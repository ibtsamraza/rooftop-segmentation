from torch.utils.data import Dataset, DataLoader
from transformers import AdamW
import torch
from torch import nn
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader
from tqdm import tqdm
import os
from PIL import Image
from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor,SegformerImageProcessor
import pandas as pd
import cv2
import numpy as np
import albumentations as aug
import matplotlib.pyplot as plt

WIDTH = 512
HEIGHT = 512


#This function in used to find edges with in our image

def auto_canny(image, sigma=0.33):
    # compute the median of the single channel pixel intensities
    v = np.median(image)

    # apply automatic Canny edge detection using the computed median
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))
    edged = cv2.Canny(image, lower, upper)

    # return the edged image
    return edged

#This function perform our pre processing '''

def transformation(image):
    # convert image to gray scale
    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
    # remove noised from image
    blur = cv2.bilateralFilter(gray, 5, 75, 75)
    kernel_sharp = np.array((
             [-2, -2, -2],
             [-2, 17, -2],
             [-2, -2, -2]), dtype='int')
    denoised_image = cv2.filter2D(blur, -1, kernel_sharp)
    #apply custom function defined above to find edges in images
    edges = auto_canny(image)
    # merge about three variable and stack them together 
    merged = cv2.merge([edges, denoised_image, gray])
    return merged

# This class load our data from directory apply preprocessed function defined above and save the images and mask as pytorc dataset

class ImageSegmentationDataset(Dataset):
    """Image segmentation dataset."""

    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.feature_extractor = feature_extractor
        self.train = train
        self.transforms = transforms

        sub_path = "train" if self.train else "val"
        self.img_dir = os.path.join(self.root_dir, sub_path,"images")
        self.ann_dir = os.path.join(self.root_dir, sub_path, "masks")
        
        # read images
        image_file_names = []
        for root, dirs, files in os.walk(self.img_dir):
            image_file_names.extend(files)
        self.images = sorted(image_file_names)
        
        # read annotations
        annotation_file_names = []
        for root, dirs, files in os.walk(self.ann_dir):
            annotation_file_names.extend(files)
        self.annotations = sorted(annotation_file_names)


        assert len(self.images) == len(self.annotations), "There must be as many images as there are segmentation maps"

    def __len__(self):
        return len(self.images)

    
    def __getitem__(self, idx):
        
        image = Image.open(os.path.join(self.img_dir, self.images[idx])).convert('RGB')
        image = np.array(image)
        # apply tranformation 
        image = transformation(image)       
        segmentation_map = Image.open(os.path.join(self.ann_dir, self.annotations[idx])).convert('L')
        segmentation_map = np.array(segmentation_map)


        if self.transforms is not None:
            augmented = self.transforms(image=image, mask=segmentation_map)
            # randomly crop + pad both image and segmentation map to same size
            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors="pt")
        else:
            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors="pt")

        for k,v in encoded_inputs.items():
            encoded_inputs[k].squeeze_() # remove batch dimension

        return encoded_inputs

# apply data augmentation o our data
transform = aug.Compose([
    aug.Flip(p=0.5),                  # Horizontal flipping
    aug.RandomRotate90(p=0.5),         # Random 90-degree rotations
    aug.Transpose(p=0.5),              # Transpose (flip vertically and horizontally)
    aug.RandomBrightnessContrast(p=0.2),  # Random changes to brightness and contrast
    aug.Blur(p=0.1),                   # Random blurring
    aug.RandomScale(p=0.2),            # Random scaling
    aug.HueSaturationValue(p=0.2),     # Random changes to hue, saturation, and value
])

root_dir = 'final_dataset/Split_Data_FullClass'
# We  import segformerimageprocessor form huggingface to tranform our imagee and masks to of the shape of model input layer
feature_extractor = SegformerImageProcessor(align=False, reduce_zero_label=False)
# We create our dataset by using the class we created above
train_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform)
valid_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=None, train=False)
# These are our classes
id2label = {0: 'background',
           1:'empty_roof',
           2:'obstacles_on_roof',
           3:'shadow_on_roof',
           4:'solar_panel'}

label2id = {v: k for k, v in id2label.items()}

train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)
valid_dataloader = DataLoader(valid_dataset, batch_size=4)
batch = next(iter(train_dataloader))

# We use a pretrained segformer model whic is pretraind on imagenet dataset. we download it and the initialize it
model = SegformerForSemanticSegmentation.from_pretrained("nvidia/mit-b3", ignore_mismatched_sizes=True,                        
                                                        num_labels=len(id2label), id2label=id2label, label2id=label2id,
                                                        eshape_last_stage=True)

#set optimizer and learning rate
optimizer = AdamW(model.parameters(), lr=0.00006)
#convert device to cuda
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

#we use this function to calculate Miou of our classes
def calculate_iou(predicted, target, num_classes, ignore_index=255):
    iou_scores = []

    for class_label in range(num_classes):
        # Move tensors to CPU
        predicted_cpu = predicted.cpu()
        target_cpu = target.cpu()

        # Flatten the tensors to simplify calculation
        predicted_flat = (predicted_cpu == class_label).flatten().numpy()
        target_flat = (target_cpu == class_label).flatten().numpy()

        # Ignore specified index
        valid_mask = target_flat != ignore_index

        # Calculate intersection and union
        intersection = np.logical_and(predicted_flat[valid_mask], target_flat[valid_mask]).sum()
        union = np.logical_or(predicted_flat[valid_mask], target_flat[valid_mask]).sum()

        # Calculate IoU for the current class
        iou_score = intersection / union if union != 0 else 0
        iou_scores.append(iou_score)

    # Return the average IoU across all classes
    return np.mean(iou_scores)

#This is our training loop which we are using to trian our model


for epoch in range(1, 14):  # loop over the dataset multiple times
    print("Epoch:", epoch)
    pbar = tqdm(train_dataloader)
    accuracies, ious, losses = [], [], []
    val_accuracies, val_ious, val_losses = [], [], []
    model.train()
    for idx, batch in enumerate(pbar):
        pixel_values = batch["pixel_values"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()

        outputs = model(pixel_values=pixel_values, labels=labels)
        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode="bilinear", align_corners=False)
        predicted = upsampled_logits.argmax(dim=1)

        mask = (labels != 255)
        pred_labels = predicted[mask].detach().cpu().numpy()
        true_labels = labels[mask].detach().cpu().numpy()
        
        
        accuracy = accuracy_score(pred_labels, true_labels)
        train_iou = calculate_iou(predicted, labels,5)


        loss = outputs.loss
        loss.backward()
        optimizer.step()

        accuracies.append(accuracy)
        ious.append(train_iou)
        losses.append(loss.item())

        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})

    else:
        model.eval()
        with torch.no_grad():
            for idx, batch in enumerate(valid_dataloader):
                pixel_values = batch["pixel_values"].to(device)
                labels = batch["labels"].to(device)

                outputs = model(pixel_values=pixel_values, labels=labels)
                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode="bilinear", align_corners=False)
                predicted = upsampled_logits.argmax(dim=1)

                mask = (labels != 255)
                pred_labels = predicted[mask].detach().cpu().numpy()
                true_labels = labels[mask].detach().cpu().numpy()

                val_accuracy = accuracy_score(pred_labels, true_labels)
                val_iou = calculate_iou(predicted, labels,5)


                val_loss = outputs.loss

                val_accuracies.append(val_accuracy)
                val_ious.append(val_iou)
                val_losses.append(val_loss.item())

    print(f"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\
           Train Loss: {sum(losses)/len(losses)}\
           Train IoU: {sum(ious)/len(ious)}\
           Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\
           Val Loss: {sum(val_losses)/len(val_losses)}\
           Val IoU: {sum(val_ious)/len(val_ious)}")

#save our segformer model
torch.save(model, 'segformer_rooftop_detection_final_model')